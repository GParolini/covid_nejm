{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the COVID-19 information published by the NEJM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The New England Journal of Medicine (NEJM), is a prestigious medical publication printed by the Massachusetts Medical Association. The journal has made freely available all its content related to the Covid-19 pandemic. This jupyter notebook explains how to:\n",
    "1. Extract the pdfs of the articles on Covid-19 published by NEJM using the BeautifulSoup library and save them in a local folder;\n",
    "2. Generate a dataframe with relevant metadata about the articles and save this information in a csv file;\n",
    "3. Extract the text from each pdf and save it in a file in a local folder;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import requests\n",
    "import urllib.request\n",
    "import os\n",
    "from os import getcwd, path\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.nejm.org/coronavirus' #url of the NEJM webpage on Covid-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response #response 200 indicates a successful request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Links Found: 1243\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all('a') #I identify all the anchors in the webpage to identify the artciles' pdfs\n",
    "print(\"Total Links Found:\",links.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only a few of the anchors in the webpage correspond to the articles related to Covid-19 by the NEJM\n",
    "\n",
    "def get_art_num(links):\n",
    "    art_num_list = []\n",
    "    for link in links:\n",
    "        if ('pdf' in link.get('href')):\n",
    "            my_link = link.get('href')\n",
    "            art_num = re.search(\"10.1056/(.+$)\",my_link)\n",
    "            art_num_list.append(art_num.group(0))\n",
    "    return art_num_list\n",
    "            \n",
    "\n",
    "my_articles_doi = get_art_num(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    }
   ],
   "source": [
    "my_articles_doi = get_art_num(links)\n",
    "\n",
    "def delete_doi(my_articles_doi):\n",
    "    art_list = []\n",
    "    for art in my_articles_doi:\n",
    "        art = art.split(\"/\")[1]\n",
    "        art_list.append(art)\n",
    "    return art_list\n",
    "        \n",
    "my_articles_nodoi = delete_doi(my_articles_doi)\n",
    "print(len(my_articles_nodoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_a = 'https://www.nejm.org/doi/pdf/10.1056/'\n",
    "\n",
    "def get_pdfs(my_articles_nodoi):\n",
    "    url_a = 'https://www.nejm.org/doi/pdf/10.1056/'\n",
    "    arg= ''\n",
    "    base_dir= [getcwd(), arg][path.isdir(arg)]\n",
    "   \n",
    "    for item in my_articles_nodoi:\n",
    "        url = url_a + item\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open (os.path.join(base_dir, \"pdfs\", item+'.pdf'), 'wb') as pdf:\n",
    "                    pdf.write(r.content)\n",
    "          \n",
    "get_pdfs(my_articles_nodoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    }
   ],
   "source": [
    "def get_info(item):\n",
    "    arg = \"\"\n",
    "    base_dir= [getcwd(), arg][path.isdir(arg)]\n",
    "    with open (os.path.join(base_dir, \"pdfs\", item+'.pdf'), 'rb') as f:\n",
    "        pdf = PdfFileReader(f)\n",
    "        info = pdf.getDocumentInfo()\n",
    "        number_of_pages = pdf.getNumPages()\n",
    "    return info\n",
    "\n",
    "\n",
    "def get_nested_dict(my_articles_nodoi):\n",
    "    nested_dict = {}\n",
    "    for art in my_articles_nodoi:\n",
    "        art_info = get_info(art)\n",
    "        art_dict = {art:art_info}\n",
    "        nested_dict.update(art_dict)\n",
    "    return nested_dict\n",
    "\n",
    "\n",
    "\n",
    "my_info_dict =get_nested_dict(my_articles_nodoi)\n",
    "\n",
    "print(len(my_info_dict))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ArtNum             CreationDate  \\\n",
      "0    NEJMp2031046  D:20201007172125-04'00'   \n",
      "1    NEJMp2025955  D:20201029102042-04'00'   \n",
      "2    NEJMp2027447  D:20201014084636-04'00'   \n",
      "3    NEJMp2025395  D:20201008123038-04'00'   \n",
      "4    NEJMp2024834  D:20201013111851-04'00'   \n",
      "..            ...                      ...   \n",
      "159  NEJMra032498  D:20031205104806-05'00'   \n",
      "160  NEJMoa030781        D:20030501202759Z   \n",
      "161  NEJMoa030747        D:20030506175543Z   \n",
      "162  NEJMoa030666        D:20030501202837Z   \n",
      "163  NEJMoa030685        D:20030501202841Z   \n",
      "\n",
      "                                                Author  \\\n",
      "0                                        Thomas H. Lee   \n",
      "1    Camila Strassle, E. Jardas, Jorge Ochoa, Benja...   \n",
      "2                                    Richard E. Leiter   \n",
      "3                                      Amrapali Maitra   \n",
      "4                      Jo Shapiro, Timothy B. McDonald   \n",
      "..                                                 ...   \n",
      "159  Peiris Joseph S.M., Yuen Kwok Y., Osterhaus Al...   \n",
      "160  Ksiazek Thomas G., Erdman Dean, Goldsmith Cynt...   \n",
      "161  Drosten Christian, Günther Stephan, Preiser Wo...   \n",
      "162  Tsang Kenneth W., Ho Pak L., Ooi Gaik C., Yee ...   \n",
      "163  Lee Nelson, Hui David, Wu Alan, Chan Paul, Cam...   \n",
      "\n",
      "                                                 Title  \\\n",
      "0    Seizing the Teachable Moment — Lessons from Ei...   \n",
      "1    Covid-19 Vaccine Trials and Incarcerated Peopl...   \n",
      "2                                              Reentry   \n",
      "3                                           Holding Up   \n",
      "4    Supporting Clinicians during Covid-19 and Beyo...   \n",
      "..                                                 ...   \n",
      "159              The Severe Acute Respiratory Syndrome   \n",
      "160  A Novel Coronavirus Associated with Severe Acu...   \n",
      "161  Identification of a Novel Coronavirus in Patie...   \n",
      "162  A Cluster of Cases of Severe Acute Respiratory...   \n",
      "163  A Major Outbreak of Severe Acute Respiratory S...   \n",
      "\n",
      "                                                  urls  \n",
      "0    https://www.nejm.org/doi/pdf/10.1056/NEJMp2031046  \n",
      "1    https://www.nejm.org/doi/pdf/10.1056/NEJMp2025955  \n",
      "2    https://www.nejm.org/doi/pdf/10.1056/NEJMp2027447  \n",
      "3    https://www.nejm.org/doi/pdf/10.1056/NEJMp2025395  \n",
      "4    https://www.nejm.org/doi/pdf/10.1056/NEJMp2024834  \n",
      "..                                                 ...  \n",
      "159  https://www.nejm.org/doi/pdf/10.1056/NEJMra032498  \n",
      "160  https://www.nejm.org/doi/pdf/10.1056/NEJMoa030781  \n",
      "161  https://www.nejm.org/doi/pdf/10.1056/NEJMoa030747  \n",
      "162  https://www.nejm.org/doi/pdf/10.1056/NEJMoa030666  \n",
      "163  https://www.nejm.org/doi/pdf/10.1056/NEJMoa030685  \n",
      "\n",
      "[164 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "dfObj = pd.DataFrame(my_info_dict).transpose()\n",
    "df = dfObj.drop(['/Creator', '/ModDate','/Trapped', '/Producer','/Subject'], axis = 1).reset_index()\n",
    "df1 = df.rename(columns={'index': 'ArtNum',\"/CreationDate\":\"CreationDate\", \"/Author\":\"Author\",\"/Title\":\"Title\"})\n",
    "url_list = ['https://www.nejm.org/doi/pdf/10.1056/'+x for x in my_articles_nodoi]\n",
    "df1['urls'] = url_list\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ArtNum           ArtType             CreationDate  \\\n",
      "0    NEJMp2031046       Perspective  D:20201007172125-04'00'   \n",
      "1    NEJMp2025955       Perspective  D:20201029102042-04'00'   \n",
      "2    NEJMp2027447       Perspective  D:20201014084636-04'00'   \n",
      "3    NEJMp2025395       Perspective  D:20201008123038-04'00'   \n",
      "4    NEJMp2024834       Perspective  D:20201013111851-04'00'   \n",
      "..            ...               ...                      ...   \n",
      "159  NEJMra032498  Current concepts  D:20031205104806-05'00'   \n",
      "160  NEJMoa030781  Original article        D:20030501202759Z   \n",
      "161  NEJMoa030747  Original article        D:20030506175543Z   \n",
      "162  NEJMoa030666  Original article        D:20030501202837Z   \n",
      "163  NEJMoa030685  Original article        D:20030501202841Z   \n",
      "\n",
      "                                                Author  \\\n",
      "0                                        Thomas H. Lee   \n",
      "1    Camila Strassle, E. Jardas, Jorge Ochoa, Benja...   \n",
      "2                                    Richard E. Leiter   \n",
      "3                                      Amrapali Maitra   \n",
      "4                      Jo Shapiro, Timothy B. McDonald   \n",
      "..                                                 ...   \n",
      "159  Peiris Joseph S.M., Yuen Kwok Y., Osterhaus Al...   \n",
      "160  Ksiazek Thomas G., Erdman Dean, Goldsmith Cynt...   \n",
      "161  Drosten Christian, Günther Stephan, Preiser Wo...   \n",
      "162  Tsang Kenneth W., Ho Pak L., Ooi Gaik C., Yee ...   \n",
      "163  Lee Nelson, Hui David, Wu Alan, Chan Paul, Cam...   \n",
      "\n",
      "                                                 Title  \\\n",
      "0    Seizing the Teachable Moment — Lessons from Ei...   \n",
      "1    Covid-19 Vaccine Trials and Incarcerated Peopl...   \n",
      "2                                              Reentry   \n",
      "3                                           Holding Up   \n",
      "4    Supporting Clinicians during Covid-19 and Beyo...   \n",
      "..                                                 ...   \n",
      "159              The Severe Acute Respiratory Syndrome   \n",
      "160  A Novel Coronavirus Associated with Severe Acu...   \n",
      "161  Identification of a Novel Coronavirus in Patie...   \n",
      "162  A Cluster of Cases of Severe Acute Respiratory...   \n",
      "163  A Major Outbreak of Severe Acute Respiratory S...   \n",
      "\n",
      "                                                  urls  \n",
      "0    https://www.nejm.org/doi/pdf/10.1056/NEJMp2031046  \n",
      "1    https://www.nejm.org/doi/pdf/10.1056/NEJMp2025955  \n",
      "2    https://www.nejm.org/doi/pdf/10.1056/NEJMp2027447  \n",
      "3    https://www.nejm.org/doi/pdf/10.1056/NEJMp2025395  \n",
      "4    https://www.nejm.org/doi/pdf/10.1056/NEJMp2024834  \n",
      "..                                                 ...  \n",
      "159  https://www.nejm.org/doi/pdf/10.1056/NEJMra032498  \n",
      "160  https://www.nejm.org/doi/pdf/10.1056/NEJMoa030781  \n",
      "161  https://www.nejm.org/doi/pdf/10.1056/NEJMoa030747  \n",
      "162  https://www.nejm.org/doi/pdf/10.1056/NEJMoa030666  \n",
      "163  https://www.nejm.org/doi/pdf/10.1056/NEJMoa030685  \n",
      "\n",
      "[164 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "art_num = df1['ArtNum'].to_list()\n",
    "\n",
    "\n",
    "def get_art_type(art_num):\n",
    "    art_type = []\n",
    "    for art in art_num:\n",
    "        if re.match(\"NEJMp\", art):\n",
    "            art_type.append(\"Perspective\")\n",
    "        elif re.match(\"NEJMcibr\", art):\n",
    "            art_type.append(\"Clinical implications of basic research\")\n",
    "        elif re.match(\"NEJMcpc\", art):\n",
    "            art_type.append(\"Case records\") \n",
    "        elif re.match(\"NEJMc\", art):\n",
    "            art_type.append(\"Correspondence\")\n",
    "        elif re.match(\"NEJMe\", art):\n",
    "            art_type.append(\"Editorial\")\n",
    "        elif re.match(\"NEJMsb\", art):\n",
    "            art_type.append(\"Sounding board\")\n",
    "        elif re.match(\"NEJMms\", art):\n",
    "            art_type.append(\"Medicine and society\")\n",
    "        elif re.match(\"NEJMoa\", art):\n",
    "            art_type.append(\"Original article\")\n",
    "        elif re.match(\"NEJMra\", art):\n",
    "            art_type.append(\"Current concepts\")\n",
    "\n",
    "    return art_type\n",
    "\n",
    "art_type = get_art_type(art_num)\n",
    "df1[\"ArtType\"] = art_type\n",
    "\n",
    "metadata = df1[['ArtNum', 'ArtType', 'CreationDate', 'Author', \"Title\", \"urls\"]]\n",
    "\n",
    "metadata.to_csv(r'metadata.csv')\n",
    "print(metadata)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (20201018)\r\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (from pdfminer.six) (3.0.4)\r\n",
      "Requirement already satisfied: cryptography in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (from pdfminer.six) (3.2.1)\r\n",
      "Requirement already satisfied: sortedcontainers in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (from pdfminer.six) (2.3.0)\r\n",
      "Requirement already satisfied: six>=1.4.1 in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (from cryptography->pdfminer.six) (1.15.0)\r\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (from cryptography->pdfminer.six) (1.14.3)\r\n",
      "Requirement already satisfied: pycparser in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (from cffi!=1.11.3,>=1.8->cryptography->pdfminer.six) (2.20)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_articles_nodoi = delete_doi(my_articles_doi)\n",
    "\n",
    "def get_txt(item):\n",
    "    output_string = StringIO()\n",
    "    arg = \"\"\n",
    "    base_dir= [getcwd(), arg][path.isdir(arg)]\n",
    "    with open (os.path.join(base_dir, \"pdfs\", item+\".pdf\"), 'rb') as f:\n",
    "        parser = PDFParser(f)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for page in PDFPage.create_pages(doc):\n",
    "            interpreter.process_page(page)\n",
    "    return output_string.getvalue()\n",
    "\n",
    "def txt_file(item, txt):\n",
    "    arg = \"\"\n",
    "    base_dir= [getcwd(), arg][path.isdir(arg)]\n",
    "    with open (os.path.join(base_dir, \"txts\", item+\".txt\"), 'w+') as f:\n",
    "        f.write(txt)\n",
    "\n",
    "for art in my_articles_nodoi:\n",
    "    my_txt = get_txt(art)\n",
    "    txt_file(art, my_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = glob.glob(\"txts/*.txt\")\n",
    "\n",
    "\n",
    "for item in txt_files:\n",
    "    filename = item.replace(item[:5], '')\n",
    "    arg = \"\"\n",
    "    base_dir= [getcwd(), arg][path.isdir(arg)]\n",
    "    with open(os.path.join(base_dir, \"txts\",filename), 'r+') as f:\n",
    "        text = f.read()\n",
    "        text = re.sub(r'-\\n(\\w+ *)', r'\\1\\n', text)\n",
    "        text = re.sub('ENGLA ND', 'ENGLAND', text)\n",
    "        text = re.sub('New England Journal of Medicine', '', text)\n",
    "        text = re.sub('Downloaded from nejm.org on', '', text)\n",
    "        text = re.sub('For personal use only.', '', text)\n",
    "        text = re.sub('No other uses without permission.', '', text)\n",
    "        text = re.sub('Copyright © 2020 Massachusetts Medical Society.', '', text)\n",
    "        text = re.sub('All rights reserved.', '', text)\n",
    "        text = re.sub(r'^n engl j med.*\\n?', '', text, flags=re.MULTILINE)\n",
    "    with open(os.path.join(base_dir, \"txts_cleaned\",filename), 'w+') as f:\n",
    "        f.write(text)\n",
    "\n",
    "\n",
    "for item in txt_files:        \n",
    "    filename = item.replace(item[:5], '')\n",
    "    with open(os.path.join(base_dir, \"txts_cleaned\",filename), \"r+\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    with open(os.path.join(base_dir, \"txts_cleaned\",filename), 'w+') as f:\n",
    "        lines = filter(lambda x: x.strip(), lines)\n",
    "        f.writelines(lines)   \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (3.8.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (from gensim) (1.5.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (from gensim) (1.19.4)\n",
      "Requirement already satisfied, skipping upgrade: requests in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /Users/giudittaparolini/anaconda3/envs/nejmenv/lib/python3.9/site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-13 15:46:35,476 : INFO : reading file NEJMp2015556.txt...\n",
      "2020-11-13 15:46:35,477 : INFO : reading file NEJMp2020076.txt...\n",
      "2020-11-13 15:46:35,479 : INFO : reading file NEJMp2010758.txt...\n",
      "2020-11-13 15:46:35,480 : INFO : reading file NEJMp2027447.txt...\n",
      "2020-11-13 15:46:35,482 : INFO : reading file NEJMc2001737.txt...\n",
      "2020-11-13 15:46:35,483 : INFO : reading file NEJMoa2001191.txt...\n",
      "2020-11-13 15:46:35,484 : INFO : reading file NEJMp2008300.txt...\n",
      "2020-11-13 15:46:35,485 : INFO : reading file NEJMp2005630.txt...\n",
      "2020-11-13 15:46:35,486 : INFO : reading file NEJMp2018846.txt...\n",
      "2020-11-13 15:46:35,488 : INFO : reading file NEJMcpc059003.txt...\n",
      "2020-11-13 15:46:35,489 : INFO : reading file NEJMc2013656.txt...\n",
      "2020-11-13 15:46:35,490 : INFO : reading file NEJMc2017424.txt...\n",
      "2020-11-13 15:46:35,491 : INFO : reading file NEJMp2005234.txt...\n",
      "2020-11-13 15:46:35,492 : INFO : reading file NEJMc2018688.txt...\n",
      "2020-11-13 15:46:35,493 : INFO : reading file NEJMc2009020.txt...\n",
      "2020-11-13 15:46:35,494 : INFO : reading file NEJMc2001468.txt...\n",
      "2020-11-13 15:46:35,496 : INFO : reading file NEJMp2008512.txt...\n",
      "2020-11-13 15:46:35,497 : INFO : reading file NEJMp2025331.txt...\n",
      "2020-11-13 15:46:35,498 : INFO : reading file NEJMp2007637.txt...\n",
      "2020-11-13 15:46:35,499 : INFO : reading file NEJMp2010377.txt...\n",
      "2020-11-13 15:46:35,500 : INFO : reading file NEJMc2009787.txt...\n",
      "2020-11-13 15:46:35,501 : INFO : reading file NEJMoa030666.txt...\n",
      "2020-11-13 15:46:35,503 : INFO : reading file NEJMcpc2004977.txt...\n",
      "2020-11-13 15:46:35,504 : INFO : reading file NEJMp2016673.txt...\n",
      "2020-11-13 15:46:35,506 : INFO : reading file NEJMp2007781.txt...\n",
      "2020-11-13 15:46:35,509 : INFO : reading file NEJMp2015024.txt...\n",
      "2020-11-13 15:46:35,511 : INFO : reading file NEJMoa1211721.txt...\n",
      "2020-11-13 15:46:35,512 : INFO : reading file NEJMc2001899.txt...\n",
      "2020-11-13 15:46:35,513 : INFO : reading file NEJMc2015630.txt...\n",
      "2020-11-13 15:46:35,514 : INFO : reading file NEJMp2024834.txt...\n",
      "2020-11-13 15:46:35,516 : INFO : reading file NEJMp2029479.txt...\n",
      "2020-11-13 15:46:35,517 : INFO : reading file NEJMc2009191.txt...\n",
      "2020-11-13 15:46:35,518 : INFO : reading file NEJMp2006115.txt...\n",
      "2020-11-13 15:46:35,522 : INFO : reading file NEJMp2000821.txt...\n",
      "2020-11-13 15:46:35,524 : INFO : reading file NEJMp2013693.txt...\n",
      "2020-11-13 15:46:35,526 : INFO : reading file NEJMc2009226.txt...\n",
      "2020-11-13 15:46:35,529 : INFO : reading file NEJMp2020926.txt...\n",
      "2020-11-13 15:46:35,534 : INFO : reading file NEJMe2001329.txt...\n",
      "2020-11-13 15:46:35,537 : INFO : reading file NEJMp2019989.txt...\n",
      "2020-11-13 15:46:35,539 : INFO : reading file NEJMc2007800.txt...\n",
      "2020-11-13 15:46:35,541 : INFO : reading file NEJMe2002387.txt...\n",
      "2020-11-13 15:46:35,546 : INFO : reading file NEJMp2018857.txt...\n",
      "2020-11-13 15:46:35,549 : INFO : reading file NEJMc2009409.txt...\n",
      "2020-11-13 15:46:35,550 : INFO : reading file NEJMoa1405858.txt...\n",
      "2020-11-13 15:46:35,552 : INFO : reading file NEJMp2016259.txt...\n",
      "2020-11-13 15:46:35,553 : INFO : reading file NEJMp2005755.txt...\n",
      "2020-11-13 15:46:35,554 : INFO : reading file NEJMp2002953.txt...\n",
      "2020-11-13 15:46:35,560 : INFO : reading file NEJMp2011418.txt...\n",
      "2020-11-13 15:46:35,562 : INFO : reading file NEJMcibr2009737.txt...\n",
      "2020-11-13 15:46:35,564 : INFO : reading file NEJMp2022011.txt...\n",
      "2020-11-13 15:46:35,566 : INFO : reading file NEJMp2005492.txt...\n",
      "2020-11-13 15:46:35,567 : INFO : reading file NEJMc2010472.txt...\n",
      "2020-11-13 15:46:35,568 : INFO : reading file NEJMp2007124.txt...\n",
      "2020-11-13 15:46:35,569 : INFO : reading file NEJMc2001621.txt...\n",
      "2020-11-13 15:46:35,570 : INFO : reading file NEJMp2004361.txt...\n",
      "2020-11-13 15:46:35,575 : INFO : reading file NEJMp2008017.txt...\n",
      "2020-11-13 15:46:35,576 : INFO : reading file NEJMc2014819.txt...\n",
      "2020-11-13 15:46:35,580 : INFO : reading file NEJMp2016822.txt...\n",
      "2020-11-13 15:46:35,582 : INFO : reading file NEJMcpc2002418.txt...\n",
      "2020-11-13 15:46:35,583 : INFO : reading file NEJMc2011592.txt...\n",
      "2020-11-13 15:46:35,584 : INFO : reading file NEJMcpc2002419.txt...\n",
      "2020-11-13 15:46:35,585 : INFO : reading file NEJMp2005687.txt...\n",
      "2020-11-13 15:46:35,586 : INFO : reading file NEJMc2010459.txt...\n",
      "2020-11-13 15:46:35,589 : INFO : reading file NEJMp2031046.txt...\n",
      "2020-11-13 15:46:35,591 : INFO : reading file NEJMp2006761.txt...\n",
      "2020-11-13 15:46:35,592 : INFO : reading file NEJMsb2021088.txt...\n",
      "2020-11-13 15:46:35,595 : INFO : reading file NEJMp2006588.txt...\n",
      "2020-11-13 15:46:35,596 : INFO : reading file NEJMp2014836.txt...\n",
      "2020-11-13 15:46:35,597 : INFO : reading file NEJMp2011027.txt...\n",
      "2020-11-13 15:46:35,599 : INFO : reading file NEJMp2008006.txt...\n",
      "2020-11-13 15:46:35,599 : INFO : reading file NEJMcpc2002421.txt...\n",
      "2020-11-13 15:46:35,601 : INFO : reading file NEJMp2002106.txt...\n",
      "2020-11-13 15:46:35,602 : INFO : reading file NEJMp2004211.txt...\n",
      "2020-11-13 15:46:35,604 : INFO : reading file NEJMoa2001282.txt...\n",
      "2020-11-13 15:46:35,605 : INFO : reading file NEJMp2009457.txt...\n",
      "2020-11-13 15:46:35,607 : INFO : reading file NEJMp2024274.txt...\n",
      "2020-11-13 15:46:35,608 : INFO : reading file NEJMp2020576.txt...\n",
      "2020-11-13 15:46:35,610 : INFO : reading file NEJMp2025395.txt...\n",
      "2020-11-13 15:46:35,611 : INFO : reading file NEJMc2005696.txt...\n",
      "2020-11-13 15:46:35,612 : INFO : reading file NEJMp2018570.txt...\n",
      "2020-11-13 15:46:35,613 : INFO : reading file NEJMcpc2002422.txt...\n",
      "2020-11-13 15:46:35,614 : INFO : reading file NEJMe2007263.txt...\n",
      "2020-11-13 15:46:35,616 : INFO : reading file NEJMc2011595.txt...\n",
      "2020-11-13 15:46:35,617 : INFO : reading file NEJMp2006376.txt...\n",
      "2020-11-13 15:46:35,618 : INFO : reading file NEJMp2005118.txt...\n",
      "2020-11-13 15:46:35,619 : INFO : reading file NEJMp2024897.txt...\n",
      "2020-11-13 15:46:35,620 : INFO : reading file NEJMc2009324.txt...\n",
      "2020-11-13 15:46:35,621 : INFO : reading file NEJMp2012114.txt...\n",
      "2020-11-13 15:46:35,623 : INFO : reading file NEJMp2006607.txt...\n",
      "2020-11-13 15:46:35,624 : INFO : reading file NEJMoa2004500.txt...\n",
      "2020-11-13 15:46:35,626 : INFO : reading file NEJMoa030747.txt...\n",
      "2020-11-13 15:46:35,628 : INFO : reading file NEJMp2012910.txt...\n",
      "2020-11-13 15:46:35,629 : INFO : reading file NEJMp2019662.txt...\n",
      "2020-11-13 15:46:35,630 : INFO : reading file NEJMsb1905390.txt...\n",
      "2020-11-13 15:46:35,631 : INFO : reading file NEJMc2010122.txt...\n",
      "2020-11-13 15:46:35,632 : INFO : reading file NEJMc2011599.txt...\n",
      "2020-11-13 15:46:35,634 : INFO : reading file NEJMp2028535.txt...\n",
      "2020-11-13 15:46:35,635 : INFO : reading file NEJMp2023312.txt...\n",
      "2020-11-13 15:46:35,636 : INFO : reading file NEJMp2003149.txt...\n",
      "2020-11-13 15:46:35,639 : INFO : reading file NEJMp2011997.txt...\n",
      "2020-11-13 15:46:35,641 : INFO : reading file NEJMp2020962.txt...\n",
      "2020-11-13 15:46:35,642 : INFO : reading file NEJMc2013400.txt...\n",
      "2020-11-13 15:46:35,644 : INFO : reading file NEJMp2007073.txt...\n",
      "2020-11-13 15:46:35,645 : INFO : reading file NEJMp2017739.txt...\n",
      "2020-11-13 15:46:35,646 : INFO : reading file NEJMp2006740.txt...\n",
      "2020-11-13 15:46:35,648 : INFO : reading file NEJMp2024046.txt...\n",
      "2020-11-13 15:46:35,649 : INFO : reading file NEJMc2011400.txt...\n",
      "2020-11-13 15:46:35,650 : INFO : reading file NEJMoa2002032.txt...\n",
      "2020-11-13 15:46:35,651 : INFO : reading file NEJMoa1401505.txt...\n",
      "2020-11-13 15:46:35,653 : INFO : reading file NEJMp2007466.txt...\n",
      "2020-11-13 15:46:35,654 : INFO : reading file NEJMp2013413.txt...\n",
      "2020-11-13 15:46:35,655 : INFO : reading file NEJMp2021264.txt...\n",
      "2020-11-13 15:46:35,656 : INFO : reading file NEJMp2004768.txt...\n",
      "2020-11-13 15:46:35,658 : INFO : reading file NEJMe2024894.txt...\n",
      "2020-11-13 15:46:35,659 : INFO : reading file NEJMp2011359.txt...\n",
      "2020-11-13 15:46:35,660 : INFO : reading file NEJMc2008597.txt...\n",
      "2020-11-13 15:46:35,661 : INFO : reading file NEJMoa1306742.txt...\n",
      "2020-11-13 15:46:35,662 : INFO : reading file NEJMc2014960.txt...\n",
      "2020-11-13 15:46:35,663 : INFO : reading file NEJMp2008797.txt...\n",
      "2020-11-13 15:46:35,664 : INFO : reading file NEJMp2021887.txt...\n",
      "2020-11-13 15:46:35,665 : INFO : reading file NEJMoa030781.txt...\n",
      "2020-11-13 15:46:35,666 : INFO : reading file NEJMc2004973.txt...\n",
      "2020-11-13 15:46:35,667 : INFO : reading file NEJMp2008193.txt...\n",
      "2020-11-13 15:46:35,668 : INFO : reading file NEJMoa2001316.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-13 15:46:35,670 : INFO : reading file NEJMc2014816.txt...\n",
      "2020-11-13 15:46:35,672 : INFO : reading file NEJMp2005689.txt...\n",
      "2020-11-13 15:46:35,674 : INFO : reading file NEJMp2000929.txt...\n",
      "2020-11-13 15:46:35,676 : INFO : reading file NEJMp2002125.txt...\n",
      "2020-11-13 15:46:35,677 : INFO : reading file NEJMp2003762.txt...\n",
      "2020-11-13 15:46:35,679 : INFO : reading file NEJMp2025955.txt...\n",
      "2020-11-13 15:46:35,680 : INFO : reading file NEJMc2001573.txt...\n",
      "2020-11-13 15:46:35,681 : INFO : reading file NEJMc2007575.txt...\n",
      "2020-11-13 15:46:35,682 : INFO : reading file NEJMc2019373.txt...\n",
      "2020-11-13 15:46:35,683 : INFO : reading file NEJMc2001272.txt...\n",
      "2020-11-13 15:46:35,684 : INFO : reading file NEJMc2010419.txt...\n",
      "2020-11-13 15:46:35,685 : INFO : reading file NEJMp2005638.txt...\n",
      "2020-11-13 15:46:35,687 : INFO : reading file NEJMp2012147.txt...\n",
      "2020-11-13 15:46:35,689 : INFO : reading file NEJMc2010418.txt...\n",
      "2020-11-13 15:46:35,690 : INFO : reading file NEJMp2022641.txt...\n",
      "2020-11-13 15:46:35,692 : INFO : reading file NEJMc2009567.txt...\n",
      "2020-11-13 15:46:35,694 : INFO : reading file NEJMms2009984.txt...\n",
      "2020-11-13 15:46:35,695 : INFO : reading file NEJMra032498.txt...\n",
      "2020-11-13 15:46:35,696 : INFO : reading file NEJMp2009405.txt...\n",
      "2020-11-13 15:46:35,697 : INFO : reading file NEJMoa1303729.txt...\n",
      "2020-11-13 15:46:35,698 : INFO : reading file NEJMp2011100.txt...\n",
      "2020-11-13 15:46:35,700 : INFO : reading file NEJMp2003539.txt...\n",
      "2020-11-13 15:46:35,701 : INFO : reading file NEJMoa030685.txt...\n",
      "2020-11-13 15:46:35,702 : INFO : reading file NEJMp2005835.txt...\n",
      "2020-11-13 15:46:35,702 : INFO : reading file NEJMp2013266.txt...\n",
      "2020-11-13 15:46:35,703 : INFO : reading file NEJMp2014455.txt...\n",
      "2020-11-13 15:46:35,705 : INFO : reading file NEJMc2011117.txt...\n",
      "2020-11-13 15:46:35,707 : INFO : reading file NEJMp2019830.txt...\n",
      "2020-11-13 15:46:35,708 : INFO : reading file NEJMc2009166.txt...\n",
      "2020-11-13 15:46:35,710 : INFO : reading file NEJMp2007028.txt...\n",
      "2020-11-13 15:46:35,711 : INFO : reading file NEJMp2014108.txt...\n",
      "2020-11-13 15:46:35,713 : INFO : reading file NEJMoa2001017.txt...\n",
      "2020-11-13 15:46:35,713 : INFO : reading file NEJMc2022860.txt...\n",
      "2020-11-13 15:46:35,714 : INFO : reading file NEJMp2025512.txt...\n",
      "2020-11-13 15:46:35,716 : INFO : reading file NEJMp2009985.txt...\n",
      "2020-11-13 15:46:35,717 : INFO : reading file NEJMp2026393.txt...\n",
      "2020-11-13 15:46:35,718 : INFO : reading file NEJMp2016293.txt...\n",
      "2020-11-13 15:46:35,719 : INFO : reading file NEJMc2010025.txt...\n",
      "2020-11-13 15:46:35,720 : INFO : reading file NEJMc2003717.txt...\n",
      "2020-11-13 15:46:35,722 : INFO : reading file NEJMp2005953.txt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    }
   ],
   "source": [
    "def read_input():\n",
    "    txt_files = glob.glob(\"txts_cleaned/*.txt\")\n",
    "    arg = \"\"\n",
    "    base_dir= [getcwd(), arg][path.isdir(arg)]\n",
    "    \n",
    "    for item in txt_files:\n",
    "        filename = item.replace(item[:13], '')\n",
    "        logging.info(\"reading file {0}...\".format(filename))\n",
    "        with open(os.path.join(base_dir, \"txts_cleaned\",filename), 'r+') as f:\n",
    "            text = f.read()\n",
    "            #logging.info (\"read {0} file\".format (filename))\n",
    "            #yield gensim.utils.simple_preprocess(text)\n",
    "            yield text\n",
    "\n",
    "documents = list(read_input())\n",
    "with open(os.path.join(base_dir, \"documents.txt\"), 'w+') as f:\n",
    "            f.writelines(str(documents))\n",
    "\n",
    "print(len(documents))\n",
    "\n",
    "#logging.info (\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-13 15:34:47,980 : INFO : collecting all words and their counts\n",
      "2020-11-13 15:34:47,981 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-11-13 15:34:48,055 : INFO : collected 20945 word types from a corpus of 329770 raw words and 164 sentences\n",
      "2020-11-13 15:34:48,055 : INFO : Loading a fresh vocabulary\n",
      "2020-11-13 15:34:48,142 : INFO : effective_min_count=1 retains 20945 unique words (100% of original 20945, drops 0)\n",
      "2020-11-13 15:34:48,143 : INFO : effective_min_count=1 leaves 329770 word corpus (100% of original 329770, drops 0)\n",
      "2020-11-13 15:34:48,193 : INFO : deleting the raw counts dictionary of 20945 items\n",
      "2020-11-13 15:34:48,194 : INFO : sample=0.001 downsamples 33 most-common words\n",
      "2020-11-13 15:34:48,194 : INFO : downsampling leaves estimated 269226 word corpus (81.6% of prior 329770)\n",
      "2020-11-13 15:34:48,237 : INFO : estimated required memory for 20945 words and 300 dimensions: 60740500 bytes\n",
      "2020-11-13 15:34:48,237 : INFO : resetting layer weights\n",
      "2020-11-13 15:34:52,682 : INFO : training model with 10 workers on 20945 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-11-13 15:34:53,001 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-13 15:34:53,014 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-13 15:34:53,037 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-13 15:34:53,038 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-13 15:34:53,043 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-13 15:34:53,063 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-13 15:34:53,064 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-13 15:34:53,068 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-13 15:34:53,075 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-13 15:34:53,080 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-13 15:34:53,080 : INFO : EPOCH - 1 : training on 329770 raw words (269188 effective words) took 0.4s, 680221 effective words/s\n",
      "2020-11-13 15:34:53,338 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-13 15:34:53,340 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-13 15:34:53,344 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-13 15:34:53,358 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-13 15:34:53,365 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-13 15:34:53,369 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-13 15:34:53,382 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-13 15:34:53,383 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-13 15:34:53,388 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-13 15:34:53,405 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-13 15:34:53,407 : INFO : EPOCH - 2 : training on 329770 raw words (269157 effective words) took 0.3s, 834616 effective words/s\n",
      "2020-11-13 15:34:53,733 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-13 15:34:53,752 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-13 15:34:53,760 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-13 15:34:53,763 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-13 15:34:53,764 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-13 15:34:53,768 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-13 15:34:53,780 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-13 15:34:53,795 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-13 15:34:53,795 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-13 15:34:53,798 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-13 15:34:53,799 : INFO : EPOCH - 3 : training on 329770 raw words (268963 effective words) took 0.4s, 698643 effective words/s\n",
      "2020-11-13 15:34:54,097 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-13 15:34:54,121 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-13 15:34:54,128 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-13 15:34:54,145 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-13 15:34:54,163 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-13 15:34:54,163 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-13 15:34:54,166 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-13 15:34:54,170 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-13 15:34:54,192 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-13 15:34:54,193 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-13 15:34:54,194 : INFO : EPOCH - 4 : training on 329770 raw words (269251 effective words) took 0.4s, 692271 effective words/s\n",
      "2020-11-13 15:34:54,456 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-13 15:34:54,459 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-13 15:34:54,490 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-13 15:34:54,501 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-13 15:34:54,517 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-13 15:34:54,526 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-13 15:34:54,527 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-13 15:34:54,534 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-13 15:34:54,535 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-13 15:34:54,544 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-13 15:34:54,545 : INFO : EPOCH - 5 : training on 329770 raw words (269189 effective words) took 0.3s, 777600 effective words/s\n",
      "2020-11-13 15:34:54,546 : INFO : training on a 1648850 raw words (1345748 effective words) took 1.9s, 722300 effective words/s\n",
      "2020-11-13 15:34:54,562 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-11-13 15:34:54,563 : INFO : training model with 10 workers on 20945 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-11-13 15:34:54,871 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-13 15:34:54,872 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-13 15:34:54,889 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-13 15:34:54,895 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-13 15:34:54,915 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-13 15:34:54,920 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-13 15:34:54,936 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-13 15:34:54,940 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-13 15:34:54,945 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-13 15:34:54,958 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-13 15:34:54,959 : INFO : EPOCH - 1 : training on 329770 raw words (269318 effective words) took 0.4s, 687991 effective words/s\n",
      "2020-11-13 15:34:55,290 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-13 15:34:55,300 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-13 15:34:55,303 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-13 15:34:55,309 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-13 15:34:55,316 : INFO : worker thread finished; awaiting finish of 5 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-13 15:34:55,318 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-13 15:34:55,325 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-13 15:34:55,333 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-13 15:34:55,334 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-13 15:34:55,334 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-13 15:34:55,335 : INFO : EPOCH - 2 : training on 329770 raw words (269372 effective words) took 0.4s, 721854 effective words/s\n",
      "2020-11-13 15:34:55,592 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-13 15:34:55,601 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-13 15:34:55,614 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-13 15:34:55,619 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-13 15:34:55,621 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-13 15:34:55,627 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-13 15:34:55,633 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-13 15:34:55,638 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-13 15:34:55,641 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-13 15:34:55,650 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-13 15:34:55,650 : INFO : EPOCH - 3 : training on 329770 raw words (269077 effective words) took 0.3s, 869387 effective words/s\n",
      "2020-11-13 15:34:55,881 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-13 15:34:55,890 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-13 15:34:55,905 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-13 15:34:55,918 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-13 15:34:55,923 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-13 15:34:55,931 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-13 15:34:55,939 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-13 15:34:55,940 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-13 15:34:55,943 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-13 15:34:55,947 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-13 15:34:55,948 : INFO : EPOCH - 4 : training on 329770 raw words (269167 effective words) took 0.3s, 915034 effective words/s\n",
      "2020-11-13 15:34:56,301 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-13 15:34:56,305 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-13 15:34:56,320 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-13 15:34:56,322 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-13 15:34:56,330 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-13 15:34:56,338 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-13 15:34:56,356 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-13 15:34:56,357 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-13 15:34:56,364 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-13 15:34:56,370 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-13 15:34:56,371 : INFO : EPOCH - 5 : training on 329770 raw words (269017 effective words) took 0.4s, 641024 effective words/s\n",
      "2020-11-13 15:34:56,678 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-13 15:34:56,679 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-13 15:34:56,680 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-13 15:34:56,681 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-13 15:34:56,685 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-13 15:34:56,688 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-13 15:34:56,697 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-13 15:34:56,706 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-13 15:34:56,716 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-13 15:34:56,724 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-13 15:34:56,725 : INFO : EPOCH - 6 : training on 329770 raw words (269070 effective words) took 0.4s, 768701 effective words/s\n",
      "2020-11-13 15:34:56,993 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-13 15:34:56,994 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-13 15:34:56,997 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-13 15:34:56,997 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-13 15:34:57,006 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-13 15:34:57,011 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-13 15:34:57,022 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-13 15:34:57,023 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-13 15:34:57,023 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-13 15:34:57,037 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-13 15:34:57,038 : INFO : EPOCH - 7 : training on 329770 raw words (269406 effective words) took 0.3s, 875875 effective words/s\n",
      "2020-11-13 15:34:57,362 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-13 15:34:57,364 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-13 15:34:57,368 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-13 15:34:57,371 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-13 15:34:57,372 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-13 15:34:57,380 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-13 15:34:57,388 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-13 15:34:57,397 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-13 15:34:57,398 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-13 15:34:57,406 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-13 15:34:57,406 : INFO : EPOCH - 8 : training on 329770 raw words (269300 effective words) took 0.4s, 736583 effective words/s\n",
      "2020-11-13 15:34:57,752 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-13 15:34:57,754 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-13 15:34:57,766 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-13 15:34:57,768 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-13 15:34:57,769 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-13 15:34:57,773 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-13 15:34:57,778 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-13 15:34:57,779 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-13 15:34:57,783 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-13 15:34:57,789 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-13 15:34:57,790 : INFO : EPOCH - 9 : training on 329770 raw words (269145 effective words) took 0.4s, 713353 effective words/s\n",
      "2020-11-13 15:34:58,116 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-13 15:34:58,119 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-13 15:34:58,126 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-13 15:34:58,130 : INFO : worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-13 15:34:58,138 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-13 15:34:58,144 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-13 15:34:58,145 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-13 15:34:58,152 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-13 15:34:58,154 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-13 15:34:58,167 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-13 15:34:58,168 : INFO : EPOCH - 10 : training on 329770 raw words (269076 effective words) took 0.4s, 721994 effective words/s\n",
      "2020-11-13 15:34:58,169 : INFO : training on a 3297700 raw words (2691948 effective words) took 3.6s, 746602 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2691948, 3297700)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec (documents, size=300, window=5, min_count=1, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'Covid-19' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-717b1f8413f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Covid-19\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#model.wv.similarity(w1=\"coronavirus\",w2=\"sars\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nejmenv/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nejmenv/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'Covid-19' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "w1 = \"Covid-19\"\n",
    "model.wv.most_similar (positive=w1)\n",
    "#model.wv.similarity(w1=\"coronavirus\",w2=\"sars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
